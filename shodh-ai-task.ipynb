{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":370089,"sourceType":"datasetVersion","datasetId":902}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install d3rlpy for Offline RL\n!pip install d3rlpy torch scikit-learn pandas numpy matplotlib seaborn --quiet\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import roc_auc_score, f1_score\nimport d3rlpy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Set device \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Setup complete. Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:30:36.479725Z","iopub.execute_input":"2025-12-09T18:30:36.480026Z","iopub.status.idle":"2025-12-09T18:32:26.794320Z","shell.execute_reply.started":"2025-12-09T18:30:36.480007Z","shell.execute_reply":"2025-12-09T18:32:26.793651Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\nkaggle-environments 1.18.0 requires gymnasium==0.29.0, but you have gymnasium 1.0.0 which is incompatible.\ndopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[2m2025-12-09 18:32.26\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRegister Shimmy environments. \u001b[0m\nSetup complete. Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# LOAD DATA \n\nFILE_PATH = '/kaggle/input/lending-club/accepted_2007_to_2018Q4.csv.gz'\n\nprint(f\"Attempting to load: {FILE_PATH}\")\n\n# We only load the columns we need to prevent RAM crashes \ncols_to_keep = [\n    'loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'sub_grade',\n    'emp_length', 'home_ownership', 'annual_inc', 'verification_status',\n    'loan_status', 'dti', 'fico_range_low', 'fico_range_high', 'open_acc',\n    'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'mort_acc', 'pub_rec_bankruptcies'\n]\n\n\n# We use compression='gzip' to handle the .gz extension correctly.\n# We limit to 500,000 rows for speed. \ntry:\n    df = pd.read_csv(FILE_PATH, usecols=cols_to_keep, compression='gzip', nrows=500000)\n    print(\"File loaded successfully.\")\nexcept FileNotFoundError:\n    print(\"ERROR: File not found. Please check the 'Add Data' panel on the right.\")\n    print(\"Make sure you added the 'All Lending Club loan data' by Nathan George.\")\n\n\n# FILTERING & CLEANING\nprint(\"Filtering for valid loan statuses...\")\n# We only want loans that have finished: Paid or Defaulted.\n# We DROP 'Current' because the outcome is unknown.\nvalid_statuses = ['Fully Paid', 'Charged Off', 'Default']\ndf = df[df['loan_status'].isin(valid_statuses)]\n\n# CREATE TARGET VARIABLE\n# 0 = Fully Paid (Good outcome)\n# 1 = Default / Charged Off (Bad outcome)\ndf['target'] = df['loan_status'].apply(lambda x: 0 if x == 'Fully Paid' else 1)\n\n# MISSING VALUES & ENCODING\n# Clean 'emp_length' (e.g. \"10+ years\" becomes 10.0)\ndf['emp_length'] = df['emp_length'].str.extract('(\\d+)').astype(float).fillna(0)\ndf.fillna(0, inplace=True)\n\n# Encode text columns to numbers\ncat_cols = ['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status']\nfor col in cat_cols:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col].astype(str))\n\nprint(f\"Data Preparation Complete.\")\nprint(f\"Final Data Shape: {df.shape}\")\nprint(f\"Columns: {list(df.columns)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:33:55.428721Z","iopub.execute_input":"2025-12-09T18:33:55.429466Z","iopub.status.idle":"2025-12-09T18:34:02.264496Z","shell.execute_reply.started":"2025-12-09T18:33:55.429433Z","shell.execute_reply":"2025-12-09T18:34:02.263860Z"}},"outputs":[{"name":"stdout","text":"Attempting to load: /kaggle/input/lending-club/accepted_2007_to_2018Q4.csv.gz\nFile loaded successfully.\nFiltering for valid loan statuses...\nData Preparation Complete.\nFinal Data Shape: (391168, 22)\nColumns: ['loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'loan_status', 'dti', 'fico_range_low', 'fico_range_high', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'mort_acc', 'pub_rec_bankruptcies', 'target']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# FEATURE ENGINEERING & SPLITTING\n\n\nprint(\"--- Engineering Rewards & Splitting Data ---\")\n\n# DEFINE REWARD FUNCTION. We define \"Value\" based on business logic.\n# Rule A: If we approve and it's paid back, we earn Interest.\n# Rule B: If we approve and it defaults, we lose the Principal amount.\n# Rule C: If we deny, we earn 0 and lose 0.\n\n# Calculate the potential outcomes for every row\ndf['potential_profit'] = df['loan_amnt'] * (df['int_rate'] / 100)\ndf['potential_loss'] = -df['loan_amnt']\n\n# Create 'reward' column based on the ACTUAL outcome in the dataset\ndf['reward'] = df.apply(lambda x: x['potential_profit'] if x['target'] == 0 else x['potential_loss'], axis=1)\n\n# PREPARE FEATURES\n# We drop columns that are not predictive features (like the target itself or the reward)\ndrop_cols = ['loan_status', 'target', 'potential_profit', 'potential_loss', 'reward']\nfeature_cols = [c for c in df.columns if c not in drop_cols]\n\n# Convert to numpy arrays\nX = df[feature_cols].values\ny = df['target'].values\nrewards = df['reward'].values\n\nprint(f\"Features selected: {len(feature_cols)}\")\n\n# SCALE FEATURES\n# Neural Networks require scaled data (mean 0, variance 1) to converge.\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 80% for Training, 20% for Testing\nX_train, X_test, y_train, y_test, r_train, r_test = train_test_split(\n    X_scaled, y, rewards, test_size=0.2, random_state=42\n)\n\nprint(\"Data successfully split.\")\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:35:11.722668Z","iopub.execute_input":"2025-12-09T18:35:11.723235Z","iopub.status.idle":"2025-12-09T18:35:14.753320Z","shell.execute_reply.started":"2025-12-09T18:35:11.723210Z","shell.execute_reply":"2025-12-09T18:35:14.752624Z"}},"outputs":[{"name":"stdout","text":"--- Engineering Rewards & Splitting Data ---\nFeatures selected: 20\nData successfully split.\nTraining set: 312934 samples\nTest set: 78234 samples\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# MODEL ARCHITECTURE\n\nclass LoanClassifier(nn.Module):\n    def __init__(self, input_dim):\n        super(LoanClassifier, self).__init__()\n        # Multi-Layer Perceptron (MLP)\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 256),   # Input layer -> Hidden 1\n            nn.ReLU(),                   # Activation\n            nn.Dropout(0.3),             # Regularization to prevent overfitting\n            nn.Linear(256, 128),         # Hidden 1 -> Hidden 2\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 1),           # Hidden 2 -> Output\n            nn.Sigmoid()                 # Sigmoid ensures output is between 0 and 1 (Probability)\n        )\n    \n    def forward(self, x):\n        return self.model(x)\n\nprint(\"Deep Learning Model Architecture defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:35:52.107797Z","iopub.execute_input":"2025-12-09T18:35:52.108354Z","iopub.status.idle":"2025-12-09T18:35:52.116622Z","shell.execute_reply.started":"2025-12-09T18:35:52.108329Z","shell.execute_reply":"2025-12-09T18:35:52.115191Z"}},"outputs":[{"name":"stdout","text":"Deep Learning Model Architecture defined.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# TRAIN SUPERVISED DL MODEL \nprint(\"--- Training Deep Learning Model ---\")\n\nmodel = LoanClassifier(X_train.shape[1]).to(device)\ncriterion = nn.BCELoss() # Binary Cross Entropy Loss for classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# DATA LOADER\ntrain_data = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\ntrain_loader = DataLoader(train_data, batch_size=2048, shuffle=True)\n\n# TRAINING \nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f}\")\n\n# EVALUATION\nprint(\"\\nEvaluating on Test Set...\")\nmodel.eval()\nwith torch.no_grad():\n    X_test_tensor = torch.FloatTensor(X_test).to(device)\n    y_pred_prob = model(X_test_tensor).cpu().numpy().squeeze()\n\n# Metrics\nauc = roc_auc_score(y_test, y_pred_prob)\ny_pred_class = (y_pred_prob > 0.5).astype(int)\nf1 = f1_score(y_test, y_pred_class)\n\nprint(f\"Final DL Results: AUC = {auc:.4f}, F1-Score = {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:35:58.303697Z","iopub.execute_input":"2025-12-09T18:35:58.304453Z","iopub.status.idle":"2025-12-09T18:36:14.800380Z","shell.execute_reply.started":"2025-12-09T18:35:58.304427Z","shell.execute_reply":"2025-12-09T18:36:14.799694Z"}},"outputs":[{"name":"stdout","text":"--- Training Deep Learning Model ---\nEpoch 1/5 - Loss: 0.4610\nEpoch 2/5 - Loss: 0.4477\nEpoch 3/5 - Loss: 0.4464\nEpoch 4/5 - Loss: 0.4460\nEpoch 5/5 - Loss: 0.4454\n\nEvaluating on Test Set...\nFinal DL Results: AUC = 0.7344, F1-Score = 0.2036\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# TRAIN OFFLINE RL AGENT \n\nprint(\"--- Training Offline RL Agent (CQL) ---\")\n\n# CREATE RL DATASET (DATA AUGMENTATION). The dataset only contains loans that were APPROVED (Action = 1).\n# To teach the agent, we must show it what happens if we DENY (Action = 0).\n# We assume Deny = Reward 0.\n\nprint(\"Constructing RL Experience Replay Buffer...\")\n\n# Real Data (Action = Approve = 1)\nobs_1 = X_train\nact_1 = np.ones(len(X_train)) # All 1s\nrew_1 = r_train\n\n# Synthetic Data (Action = Deny = 0)\n# We duplicate the observations but assign Action 0 and Reward 0\nobs_0 = X_train\nact_0 = np.zeros(len(X_train)) # All 0s\nrew_0 = np.zeros(len(X_train)) \n\n# Combine them\nobs_rl = np.vstack([obs_1, obs_0])\nact_rl = np.concatenate([act_1, act_0])\nrew_rl = np.concatenate([rew_1, rew_0])\nterminals = np.ones(len(obs_rl)) # All episodes end after 1 step\n\n# Create d3rlpy Dataset object\ndataset = d3rlpy.dataset.MDPDataset(\n    observations=obs_rl,\n    actions=act_rl,\n    rewards=rew_rl,\n    terminals=terminals,\n)\n\n# CONFIGURE ALGORITHM\n# We use Discrete CQL (Conservative Q-Learning)\ncql = d3rlpy.algos.DiscreteCQLConfig(\n    batch_size=2048,\n    learning_rate=1e-4,\n).create(device=True if torch.cuda.is_available() else False)\n\n# TRAIN AGENT\nprint(\"Starting Training (this may take a few minutes)...\")\n\n\ncql.fit(\n    dataset,\n    n_steps=10000\n)\n\nprint(\"RL Agent Training Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:37:53.353107Z","iopub.execute_input":"2025-12-09T18:37:53.353705Z","iopub.status.idle":"2025-12-09T18:45:29.084220Z","shell.execute_reply.started":"2025-12-09T18:37:53.353677Z","shell.execute_reply":"2025-12-09T18:45:29.083611Z"}},"outputs":[{"name":"stdout","text":"--- Training Offline RL Agent (CQL) ---\nConstructing RL Experience Replay Buffer...\n\u001b[2m2025-12-09 18:37.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSignatures have been automatically determined.\u001b[0m \u001b[36maction_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(1,)])\u001b[0m \u001b[36mobservation_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(20,)])\u001b[0m \u001b[36mreward_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(1,)])\u001b[0m\n\u001b[2m2025-12-09 18:37.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction-space has been automatically determined.\u001b[0m \u001b[36maction_space\u001b[0m=\u001b[35m<ActionSpace.DISCRETE: 2>\u001b[0m\n\u001b[2m2025-12-09 18:37.57\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction size has been automatically determined.\u001b[0m \u001b[36maction_size\u001b[0m=\u001b[35m2\u001b[0m\nStarting Training (this may take a few minutes)...\n\u001b[2m2025-12-09 18:37.58\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mdataset info                  \u001b[0m \u001b[36mdataset_info\u001b[0m=\u001b[35mDatasetInfo(observation_signature=Signature(dtype=[dtype('float64')], shape=[(20,)]), action_signature=Signature(dtype=[dtype('float64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=2)\u001b[0m\n\u001b[2m2025-12-09 18:37.58\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mBuilding models...            \u001b[0m\n\u001b[2m2025-12-09 18:37.58\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mModels have been built.       \u001b[0m\n\u001b[2m2025-12-09 18:37.58\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDirectory is created at d3rlpy_logs/DiscreteCQL_20251209183758\u001b[0m\n\u001b[2m2025-12-09 18:37.58\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mParameters                    \u001b[0m \u001b[36mparams\u001b[0m=\u001b[35m{'observation_shape': [20], 'action_size': 2, 'config': {'type': 'discrete_cql', 'params': {'batch_size': 2048, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'compile_graph': False, 'learning_rate': 0.0001, 'optim_factory': {'type': 'adam', 'params': {'clip_grad_norm': None, 'lr_scheduler_factory': {'type': 'none', 'params': {}}, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'n_critics': 1, 'target_update_interval': 8000, 'alpha': 1.0}}}\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/1:   0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0cec509f5c0496280b421998494d435"}},"metadata":{}},{"name":"stdout","text":"\u001b[2m2025-12-09 18:45.29\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDiscreteCQL_20251209183758: epoch=1 step=10000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.03880418281555176, 'time_algorithm_update': 0.005839399576187134, 'loss': 2275.5941565063476, 'td_loss': 2274.540419177246, 'conservative_loss': 1.0537365218222141, 'time_step': 0.04485613431930542}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m10000\u001b[0m\n\u001b[2m2025-12-09 18:45.29\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DiscreteCQL_20251209183758/model_10000.d3\u001b[0m\nRL Agent Training Complete.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# FINAL COMPARISON & REPORT\n\nprint(\"--- Comparative Analysis: DL vs RL ---\")\n\n# DECISIONS ON TEST SET\n# DL Strategy: Deny if Risk > 50% \ndl_actions = np.where(y_pred_prob > 0.5, 0, 1) # 0=Deny, 1=Approve\n\n# RL Strategy: Agent predicts the action with highest Q-Value\nrl_actions = cql.predict(X_test)\n\n# PORTFOLIO VALUE (This calculates how much money each strategy would have made on the test set)\ndef get_portfolio_value(actions, actual_rewards):\n    total = 0\n    for act, r in zip(actions, actual_rewards):\n        if act == 1:\n            total += r\n        # If act == 0 (Deny), reward is 0, so we add nothing.\n    return total\n\ndl_val = get_portfolio_value(dl_actions, r_test)\nrl_val = get_portfolio_value(rl_actions, r_test)\n\nprint(f\"Total Portfolio Value (Deep Learning):  ${dl_val:,.2f}\")\nprint(f\"Total Portfolio Value (Reinforcement Learning): ${rl_val:,.2f}\")\n\n# ANALYZE DISAGREEMENTS\n# We want to find cases where the RL agent APPROVED (1) but the DL model DENIED (0).\n# This usually happens when the Interest Rate is high enough to justify the Risk.\ndisagreements = np.where((rl_actions == 1) & (dl_actions == 0))[0]\n\nprint(f\"\\nNumber of loans where RL Approves but DL Denies: {len(disagreements)}\")\n\nif len(disagreements) > 0:\n    #top 3 examples\n    print(\"\\n--- Insight: Why did RL approve these? ---\")\n    for i in range(min(3, len(disagreements))):\n        idx = disagreements[i]\n        print(f\"\\nLoan Example #{idx}\")\n        print(f\"  DL Predicted Default Probability: {y_pred_prob[idx]*100:.2f}% (High Risk)\")\n        print(f\"  Actual Profit/Loss (if approved): ${r_test[idx]:.2f}\")\n        \n        # Note: Since we scaled X, we can't easily see the original Interest Rate here.\n        # But generally, RL approves high-risk loans only if the reward (Interest) is massive.\n        if r_test[idx] > 0:\n            print(\"  OUTCOME: The loan was actually PAID back. RL was right!\")\n        else:\n            print(\"  OUTCOME: The loan Defaulted. RL took a risk and lost.\")\nelse:\n    print(\"No disagreements found in this test batch.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:45:39.422301Z","iopub.execute_input":"2025-12-09T18:45:39.423067Z","iopub.status.idle":"2025-12-09T18:45:39.480604Z","shell.execute_reply.started":"2025-12-09T18:45:39.423039Z","shell.execute_reply":"2025-12-09T18:45:39.479925Z"}},"outputs":[{"name":"stdout","text":"--- Comparative Analysis: DL vs RL ---\nTotal Portfolio Value (Deep Learning):  $-113,689,828.21\nTotal Portfolio Value (Reinforcement Learning): $-110,073,573.73\n\nNumber of loans where RL Approves but DL Denies: 273\n\n--- Insight: Why did RL approve these? ---\n\nLoan Example #133\n  DL Predicted Default Probability: 50.09% (High Risk)\n  Actual Profit/Loss (if approved): $1853.60\n  OUTCOME: The loan was actually PAID back. RL was right!\n\nLoan Example #271\n  DL Predicted Default Probability: 54.07% (High Risk)\n  Actual Profit/Loss (if approved): $2979.00\n  OUTCOME: The loan was actually PAID back. RL was right!\n\nLoan Example #523\n  DL Predicted Default Probability: 50.36% (High Risk)\n  Actual Profit/Loss (if approved): $-12000.00\n  OUTCOME: The loan Defaulted. RL took a risk and lost.\n","output_type":"stream"}],"execution_count":8}]}